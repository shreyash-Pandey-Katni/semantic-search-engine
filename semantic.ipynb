{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import json\n",
    "import pymongo\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import faiss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer('all-mpnet-base-v2', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('dataNew.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:18<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "corpus = list()\n",
    "hrefList = list()\n",
    "for i in tqdm(data):\n",
    "    doc = nlp(data[i]['text'])\n",
    "    sents = [sent.text for sent in doc.sents]\n",
    "    corpus.extend(sents)\n",
    "    hrefList.extend([data[i]['link'] * len(sents)])\n",
    "    # corpus.extend([data[i]['text']])\n",
    "    # hrefList.extend([data[i]['link']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "476"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 15/15 [00:00<00:00, 44.85it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = min(5, len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"how to update API by importing file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = embedder.encode(query, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 10\n",
    "DELIMITER = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('data.json', 'r'))\n",
    "corpus = list()\n",
    "hrefList = list()\n",
    "for i in data:\n",
    "    corpus.extend(data[i]['data'])\n",
    "    hrefList.extend([data[i]['href']]*len(data[i]['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 15/15 [00:00<00:00, 41.73it/s]\n"
     ]
    }
   ],
   "source": [
    "faissModel = SentenceTransformer(\n",
    "    'all-mpnet-base-v2', device='cuda')\n",
    "faissEmbedding = faissModel.encode(corpus, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 476 points to 100 centroids: please provide at least 3900 training points\n",
      "WARNING clustering 476 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 476 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 476 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 476 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 476 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 476 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 476 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 476 points to 256 centroids: please provide at least 9984 training points\n"
     ]
    }
   ],
   "source": [
    "composite_index = faiss.index_factory(faissEmbedding.shape[1], \"IVF100,PQ8\")\n",
    "\n",
    "composite_index.train(faissEmbedding)\n",
    "composite_index.add(faissEmbedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of composite_index 476\n"
     ]
    }
   ],
   "source": [
    "print('size of composite_index', composite_index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the total number of vectors in the composite index\n",
    "num_vectors = composite_index.ntotal\n",
    "\n",
    "# Set the chunk size\n",
    "chunk_size = 10\n",
    "\n",
    "# Calculate the number of chunks\n",
    "num_chunks = (num_vectors + chunk_size - 1) // chunk_size\n",
    "\n",
    "# Divide the composite index into chunks and save them separately\n",
    "for i in range(num_chunks):\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = min((i + 1) * chunk_size, num_vectors)\n",
    "    chunk_index = faiss.IndexFlatL2(composite_index.d)\n",
    "\n",
    "    # Copy vectors from the composite index to the chunk index\n",
    "    chunk_index.add(composite_index.reconstruct_n(start_idx, end_idx - start_idx))\n",
    "\n",
    "    # Save the chunk index to disk\n",
    "    filename = f\"chunk_{i}.index\"\n",
    "    faiss.write_index(chunk_index, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Chunks: 100%|██████████| 48/48 [00:00<00:00, 638.10it/s]\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "from pymongo import MongoClient\n",
    "from gridfs import GridFS\n",
    "from tqdm import tqdm\n",
    "import io\n",
    "\n",
    "# MongoDB connection\n",
    "client = MongoClient(\"mongodb://localhost:27017\")\n",
    "db = client[\"your_database_name\"]\n",
    "fs = GridFS(db, collection=\"index_chunks\")\n",
    "\n",
    "# Load the composite index from disk\n",
    "# composite_index = faiss.read_index(\"composite_index.index\")\n",
    "\n",
    "# Get the total number of vectors in the composite index\n",
    "num_vectors = composite_index.ntotal\n",
    "\n",
    "# Set the chunk size\n",
    "chunk_size = 10\n",
    "\n",
    "# Calculate the number of chunks\n",
    "num_chunks = (num_vectors + chunk_size - 1) // chunk_size\n",
    "\n",
    "# Divide the composite index into chunks and save them in MongoDB with a progress bar\n",
    "with tqdm(total=num_chunks, desc=\"Saving Chunks\") as pbar:\n",
    "    for i in range(num_chunks):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = min((i + 1) * chunk_size, num_vectors)\n",
    "\n",
    "        # Create a new index for the chunk\n",
    "        chunk_index = faiss.IndexFlatL2(composite_index.d)\n",
    "\n",
    "        # Copy vectors from the composite index to the chunk index\n",
    "        chunk_index.add(composite_index.reconstruct_n(start_idx, end_idx - start_idx))\n",
    "\n",
    "        # Serialize the chunk index to bytes\n",
    "        index_data = faiss.serialize_index(chunk_index)\n",
    "\n",
    "        # Create a file-like object from the serialized index data\n",
    "        index_file = io.BytesIO(index_data)\n",
    "\n",
    "        # Save the chunk index in MongoDB using GridFS\n",
    "        fs.put(index_file, filename=f\"chunk_{i}.index\")\n",
    "\n",
    "        pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading chunk 48, error: no version -1 for filename 'chunk_48.index'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55513/2399424175.py:14: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  for i in range(fs.find().count()):\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "from pymongo import MongoClient\n",
    "from gridfs import GridFS\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "# MongoDB connection\n",
    "client = MongoClient(\"mongodb://localhost:27017\")\n",
    "db = client[\"your_database_name\"]\n",
    "fs = GridFS(db, collection=\"index_chunks\")\n",
    "\n",
    "# Retrieve the index chunks from MongoDB and load them into Faiss indexes\n",
    "index_chunks = []\n",
    "for i in range(fs.find().count()):\n",
    "    try:\n",
    "        # Read the file data from GridFS\n",
    "        index_file = fs.get_last_version(filename=f\"chunk_{i}.index\")\n",
    "\n",
    "        # Load the index data from the file-like object\n",
    "        index_data = index_file.read()\n",
    "\n",
    "        index_data = np.frombuffer(index_data, dtype=np.uint8)\n",
    "        \n",
    "        # Load the index from the index data\n",
    "        index = faiss.deserialize_index(index_data)\n",
    "\n",
    "        # Add the index to the list of chunks\n",
    "        index_chunks.append(index)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading chunk {i}, error: {e}\")\n",
    "    # # Read the file data from GridFS\n",
    "    # index_file = fs.get_last_version(filename=f\"chunk_{i}.index\")\n",
    "    \n",
    "    # # Load the index data from the file-like object\n",
    "    # index_data = index_file.read()\n",
    "    \n",
    "    # index_data = np.frombuffer(index_data, dtype=np.uint8)\n",
    "    \n",
    "    # # Load the index from the index data\n",
    "    # index = faiss.deserialize_index(index_data)\n",
    "    \n",
    "    # # Add the index to the list of chunks\n",
    "    # index_chunks.append(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.14830208, 0.48746973, 0.62839204, 1.1104012 , 1.2427244 ]],\n",
       "       dtype=float32),\n",
       " array([[0, 2, 7, 5, 9]]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_chunks[0].search(faissEmbedding[0:1], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0ff842492d3b27893a4a77d7bd2f4a30fac711420d42d094c025bdf6b54a0b49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
